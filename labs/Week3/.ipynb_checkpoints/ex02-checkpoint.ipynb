{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=day3_ex02, master=local[*]) created by __init__ at <ipython-input-1-fa5aa2bdf932>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fa5aa2bdf932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'day3_ex02'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    314\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 316\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=day3_ex02, master=local[*]) created by __init__ at <ipython-input-1-fa5aa2bdf932>:7 "
     ]
    }
   ],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "sc = pyspark.SparkContext(appName='day3_ex02').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../Data/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " '',\n",
       " '<http://spark.apache.org/>',\n",
       " '',\n",
       " '',\n",
       " '## Online Documentation',\n",
       " '',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n",
       " 'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       " 'This README file only contains basic setup instructions.',\n",
       " '',\n",
       " '## Building Spark',\n",
       " '',\n",
       " 'Spark is built using [Apache Maven](http://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '',\n",
       " '    build/mvn -DskipTests clean package',\n",
       " '',\n",
       " '(You do not need to do this if you downloaded a pre-built package.)',\n",
       " '',\n",
       " 'You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       " 'More detailed documentation is available from the project site, at',\n",
       " '[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For developing Spark using an IDE, see [Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)',\n",
       " 'and [IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).',\n",
       " '',\n",
       " '## Interactive Scala Shell',\n",
       " '',\n",
       " 'The easiest way to start using Spark is through the Scala shell:',\n",
       " '',\n",
       " '    ./bin/spark-shell',\n",
       " '',\n",
       " 'Try the following command, which should return 1000:',\n",
       " '',\n",
       " '    scala> sc.parallelize(1 to 1000).count()',\n",
       " '',\n",
       " '## Interactive Python Shell',\n",
       " '',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " '',\n",
       " '    ./bin/pyspark',\n",
       " '',\n",
       " 'And run the following command, which should also return 1000:',\n",
       " '',\n",
       " '    >>> sc.parallelize(range(1000)).count()',\n",
       " '',\n",
       " '## Example Programs',\n",
       " '',\n",
       " 'Spark also comes with several sample programs in the `examples` directory.',\n",
       " 'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       " '',\n",
       " '    ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'will run the Pi example locally.',\n",
       " '',\n",
       " 'You can set the MASTER environment variable when running examples to submit',\n",
       " 'examples to a cluster. This can be a mesos:// or spark:// URL,',\n",
       " '\"yarn\" to run on YARN, and \"local\" to run',\n",
       " 'locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       " 'can also use an abbreviated class name if the class is in the `examples`',\n",
       " 'package. For instance:',\n",
       " '',\n",
       " '    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'Many of the example programs print usage help if no params are given.',\n",
       " '',\n",
       " '## Running Tests',\n",
       " '',\n",
       " 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       " 'can be run using:',\n",
       " '',\n",
       " '    ./dev/run-tests',\n",
       " '',\n",
       " 'Please see the guidance on how to',\n",
       " '[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       " '',\n",
       " '## A Note About Hadoop Versions',\n",
       " '',\n",
       " 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       " 'storage systems. Because the protocols have changed in different versions of',\n",
       " 'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       " '',\n",
       " 'Please refer to the build documentation at',\n",
       " '[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       " 'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       " 'building for particular Hive and Hive Thriftserver distributions.',\n",
       " '',\n",
       " '## Configuration',\n",
       " '',\n",
       " 'Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in the online documentation for an overview on how to configure Spark.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 02-1 : generate a list of words within multi level structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.map(lambda line : line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " '',\n",
       " '<http://spark.apache.org/>',\n",
       " '',\n",
       " '',\n",
       " '## Online Documentation',\n",
       " '',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n",
       " 'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       " 'This README file only contains basic setup instructions.',\n",
       " '',\n",
       " '## Building Spark',\n",
       " '',\n",
       " 'Spark is built using [Apache Maven](http://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '',\n",
       " '    build/mvn -DskipTests clean package',\n",
       " '',\n",
       " '(You do not need to do this if you downloaded a pre-built package.)',\n",
       " '',\n",
       " 'You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       " 'More detailed documentation is available from the project site, at',\n",
       " '[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For developing Spark using an IDE, see [Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)',\n",
       " 'and [IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).',\n",
       " '',\n",
       " '## Interactive Scala Shell',\n",
       " '',\n",
       " 'The easiest way to start using Spark is through the Scala shell:',\n",
       " '',\n",
       " '    ./bin/spark-shell',\n",
       " '',\n",
       " 'Try the following command, which should return 1000:',\n",
       " '',\n",
       " '    scala> sc.parallelize(1 to 1000).count()',\n",
       " '',\n",
       " '## Interactive Python Shell',\n",
       " '',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " '',\n",
       " '    ./bin/pyspark',\n",
       " '',\n",
       " 'And run the following command, which should also return 1000:',\n",
       " '',\n",
       " '    >>> sc.parallelize(range(1000)).count()',\n",
       " '',\n",
       " '## Example Programs',\n",
       " '',\n",
       " 'Spark also comes with several sample programs in the `examples` directory.',\n",
       " 'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       " '',\n",
       " '    ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'will run the Pi example locally.',\n",
       " '',\n",
       " 'You can set the MASTER environment variable when running examples to submit',\n",
       " 'examples to a cluster. This can be a mesos:// or spark:// URL,',\n",
       " '\"yarn\" to run on YARN, and \"local\" to run',\n",
       " 'locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       " 'can also use an abbreviated class name if the class is in the `examples`',\n",
       " 'package. For instance:',\n",
       " '',\n",
       " '    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'Many of the example programs print usage help if no params are given.',\n",
       " '',\n",
       " '## Running Tests',\n",
       " '',\n",
       " 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       " 'can be run using:',\n",
       " '',\n",
       " '    ./dev/run-tests',\n",
       " '',\n",
       " 'Please see the guidance on how to',\n",
       " '[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       " '',\n",
       " '## A Note About Hadoop Versions',\n",
       " '',\n",
       " 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       " 'storage systems. Because the protocols have changed in different versions of',\n",
       " 'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       " '',\n",
       " 'Please refer to the build documentation at',\n",
       " '[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       " 'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       " 'building for particular Hive and Hive Thriftserver distributions.',\n",
       " '',\n",
       " '## Configuration',\n",
       " '',\n",
       " 'Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in the online documentation for an overview on how to configure Spark.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#', 'Apache', 'Spark'],\n",
       " [],\n",
       " ['Spark',\n",
       "  'is',\n",
       "  'a',\n",
       "  'fast',\n",
       "  'and',\n",
       "  'general',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'system',\n",
       "  'for',\n",
       "  'Big',\n",
       "  'Data.',\n",
       "  'It',\n",
       "  'provides'],\n",
       " ['high-level',\n",
       "  'APIs',\n",
       "  'in',\n",
       "  'Scala,',\n",
       "  'Java,',\n",
       "  'Python,',\n",
       "  'and',\n",
       "  'R,',\n",
       "  'and',\n",
       "  'an',\n",
       "  'optimized',\n",
       "  'engine',\n",
       "  'that'],\n",
       " ['supports',\n",
       "  'general',\n",
       "  'computation',\n",
       "  'graphs',\n",
       "  'for',\n",
       "  'data',\n",
       "  'analysis.',\n",
       "  'It',\n",
       "  'also',\n",
       "  'supports',\n",
       "  'a'],\n",
       " ['rich',\n",
       "  'set',\n",
       "  'of',\n",
       "  'higher-level',\n",
       "  'tools',\n",
       "  'including',\n",
       "  'Spark',\n",
       "  'SQL',\n",
       "  'for',\n",
       "  'SQL',\n",
       "  'and',\n",
       "  'DataFrames,'],\n",
       " ['MLlib',\n",
       "  'for',\n",
       "  'machine',\n",
       "  'learning,',\n",
       "  'GraphX',\n",
       "  'for',\n",
       "  'graph',\n",
       "  'processing,'],\n",
       " ['and', 'Spark', 'Streaming', 'for', 'stream', 'processing.'],\n",
       " [],\n",
       " ['<http://spark.apache.org/>'],\n",
       " [],\n",
       " [],\n",
       " ['##', 'Online', 'Documentation'],\n",
       " [],\n",
       " ['You',\n",
       "  'can',\n",
       "  'find',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'Spark',\n",
       "  'documentation,',\n",
       "  'including',\n",
       "  'a',\n",
       "  'programming'],\n",
       " ['guide,',\n",
       "  'on',\n",
       "  'the',\n",
       "  '[project',\n",
       "  'web',\n",
       "  'page](http://spark.apache.org/documentation.html)'],\n",
       " ['and',\n",
       "  '[project',\n",
       "  'wiki](https://cwiki.apache.org/confluence/display/SPARK).'],\n",
       " ['This',\n",
       "  'README',\n",
       "  'file',\n",
       "  'only',\n",
       "  'contains',\n",
       "  'basic',\n",
       "  'setup',\n",
       "  'instructions.'],\n",
       " [],\n",
       " ['##', 'Building', 'Spark'],\n",
       " [],\n",
       " ['Spark',\n",
       "  'is',\n",
       "  'built',\n",
       "  'using',\n",
       "  '[Apache',\n",
       "  'Maven](http://maven.apache.org/).'],\n",
       " ['To', 'build', 'Spark', 'and', 'its', 'example', 'programs,', 'run:'],\n",
       " [],\n",
       " ['build/mvn', '-DskipTests', 'clean', 'package'],\n",
       " [],\n",
       " ['(You',\n",
       "  'do',\n",
       "  'not',\n",
       "  'need',\n",
       "  'to',\n",
       "  'do',\n",
       "  'this',\n",
       "  'if',\n",
       "  'you',\n",
       "  'downloaded',\n",
       "  'a',\n",
       "  'pre-built',\n",
       "  'package.)'],\n",
       " [],\n",
       " ['You',\n",
       "  'can',\n",
       "  'build',\n",
       "  'Spark',\n",
       "  'using',\n",
       "  'more',\n",
       "  'than',\n",
       "  'one',\n",
       "  'thread',\n",
       "  'by',\n",
       "  'using',\n",
       "  'the',\n",
       "  '-T',\n",
       "  'option',\n",
       "  'with',\n",
       "  'Maven,',\n",
       "  'see',\n",
       "  '[\"Parallel',\n",
       "  'builds',\n",
       "  'in',\n",
       "  'Maven',\n",
       "  '3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'],\n",
       " ['More',\n",
       "  'detailed',\n",
       "  'documentation',\n",
       "  'is',\n",
       "  'available',\n",
       "  'from',\n",
       "  'the',\n",
       "  'project',\n",
       "  'site,',\n",
       "  'at'],\n",
       " ['[\"Building',\n",
       "  'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).'],\n",
       " ['For',\n",
       "  'developing',\n",
       "  'Spark',\n",
       "  'using',\n",
       "  'an',\n",
       "  'IDE,',\n",
       "  'see',\n",
       "  '[Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)'],\n",
       " ['and',\n",
       "  '[IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).'],\n",
       " [],\n",
       " ['##', 'Interactive', 'Scala', 'Shell'],\n",
       " [],\n",
       " ['The',\n",
       "  'easiest',\n",
       "  'way',\n",
       "  'to',\n",
       "  'start',\n",
       "  'using',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'through',\n",
       "  'the',\n",
       "  'Scala',\n",
       "  'shell:'],\n",
       " [],\n",
       " ['./bin/spark-shell'],\n",
       " [],\n",
       " ['Try', 'the', 'following', 'command,', 'which', 'should', 'return', '1000:'],\n",
       " [],\n",
       " ['scala>', 'sc.parallelize(1', 'to', '1000).count()'],\n",
       " [],\n",
       " ['##', 'Interactive', 'Python', 'Shell'],\n",
       " [],\n",
       " ['Alternatively,',\n",
       "  'if',\n",
       "  'you',\n",
       "  'prefer',\n",
       "  'Python,',\n",
       "  'you',\n",
       "  'can',\n",
       "  'use',\n",
       "  'the',\n",
       "  'Python',\n",
       "  'shell:'],\n",
       " [],\n",
       " ['./bin/pyspark'],\n",
       " [],\n",
       " ['And',\n",
       "  'run',\n",
       "  'the',\n",
       "  'following',\n",
       "  'command,',\n",
       "  'which',\n",
       "  'should',\n",
       "  'also',\n",
       "  'return',\n",
       "  '1000:'],\n",
       " [],\n",
       " ['>>>', 'sc.parallelize(range(1000)).count()'],\n",
       " [],\n",
       " ['##', 'Example', 'Programs'],\n",
       " [],\n",
       " ['Spark',\n",
       "  'also',\n",
       "  'comes',\n",
       "  'with',\n",
       "  'several',\n",
       "  'sample',\n",
       "  'programs',\n",
       "  'in',\n",
       "  'the',\n",
       "  '`examples`',\n",
       "  'directory.'],\n",
       " ['To',\n",
       "  'run',\n",
       "  'one',\n",
       "  'of',\n",
       "  'them,',\n",
       "  'use',\n",
       "  '`./bin/run-example',\n",
       "  '<class>',\n",
       "  '[params]`.',\n",
       "  'For',\n",
       "  'example:'],\n",
       " [],\n",
       " ['./bin/run-example', 'SparkPi'],\n",
       " [],\n",
       " ['will', 'run', 'the', 'Pi', 'example', 'locally.'],\n",
       " [],\n",
       " ['You',\n",
       "  'can',\n",
       "  'set',\n",
       "  'the',\n",
       "  'MASTER',\n",
       "  'environment',\n",
       "  'variable',\n",
       "  'when',\n",
       "  'running',\n",
       "  'examples',\n",
       "  'to',\n",
       "  'submit'],\n",
       " ['examples',\n",
       "  'to',\n",
       "  'a',\n",
       "  'cluster.',\n",
       "  'This',\n",
       "  'can',\n",
       "  'be',\n",
       "  'a',\n",
       "  'mesos://',\n",
       "  'or',\n",
       "  'spark://',\n",
       "  'URL,'],\n",
       " ['\"yarn\"', 'to', 'run', 'on', 'YARN,', 'and', '\"local\"', 'to', 'run'],\n",
       " ['locally',\n",
       "  'with',\n",
       "  'one',\n",
       "  'thread,',\n",
       "  'or',\n",
       "  '\"local[N]\"',\n",
       "  'to',\n",
       "  'run',\n",
       "  'locally',\n",
       "  'with',\n",
       "  'N',\n",
       "  'threads.',\n",
       "  'You'],\n",
       " ['can',\n",
       "  'also',\n",
       "  'use',\n",
       "  'an',\n",
       "  'abbreviated',\n",
       "  'class',\n",
       "  'name',\n",
       "  'if',\n",
       "  'the',\n",
       "  'class',\n",
       "  'is',\n",
       "  'in',\n",
       "  'the',\n",
       "  '`examples`'],\n",
       " ['package.', 'For', 'instance:'],\n",
       " [],\n",
       " ['MASTER=spark://host:7077', './bin/run-example', 'SparkPi'],\n",
       " [],\n",
       " ['Many',\n",
       "  'of',\n",
       "  'the',\n",
       "  'example',\n",
       "  'programs',\n",
       "  'print',\n",
       "  'usage',\n",
       "  'help',\n",
       "  'if',\n",
       "  'no',\n",
       "  'params',\n",
       "  'are',\n",
       "  'given.'],\n",
       " [],\n",
       " ['##', 'Running', 'Tests'],\n",
       " [],\n",
       " ['Testing',\n",
       "  'first',\n",
       "  'requires',\n",
       "  '[building',\n",
       "  'Spark](#building-spark).',\n",
       "  'Once',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'built,',\n",
       "  'tests'],\n",
       " ['can', 'be', 'run', 'using:'],\n",
       " [],\n",
       " ['./dev/run-tests'],\n",
       " [],\n",
       " ['Please', 'see', 'the', 'guidance', 'on', 'how', 'to'],\n",
       " ['[run',\n",
       "  'tests',\n",
       "  'for',\n",
       "  'a',\n",
       "  'module,',\n",
       "  'or',\n",
       "  'individual',\n",
       "  'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).'],\n",
       " [],\n",
       " ['##', 'A', 'Note', 'About', 'Hadoop', 'Versions'],\n",
       " [],\n",
       " ['Spark',\n",
       "  'uses',\n",
       "  'the',\n",
       "  'Hadoop',\n",
       "  'core',\n",
       "  'library',\n",
       "  'to',\n",
       "  'talk',\n",
       "  'to',\n",
       "  'HDFS',\n",
       "  'and',\n",
       "  'other',\n",
       "  'Hadoop-supported'],\n",
       " ['storage',\n",
       "  'systems.',\n",
       "  'Because',\n",
       "  'the',\n",
       "  'protocols',\n",
       "  'have',\n",
       "  'changed',\n",
       "  'in',\n",
       "  'different',\n",
       "  'versions',\n",
       "  'of'],\n",
       " ['Hadoop,',\n",
       "  'you',\n",
       "  'must',\n",
       "  'build',\n",
       "  'Spark',\n",
       "  'against',\n",
       "  'the',\n",
       "  'same',\n",
       "  'version',\n",
       "  'that',\n",
       "  'your',\n",
       "  'cluster',\n",
       "  'runs.'],\n",
       " [],\n",
       " ['Please', 'refer', 'to', 'the', 'build', 'documentation', 'at'],\n",
       " ['[\"Specifying',\n",
       "  'the',\n",
       "  'Hadoop',\n",
       "  'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)'],\n",
       " ['for',\n",
       "  'detailed',\n",
       "  'guidance',\n",
       "  'on',\n",
       "  'building',\n",
       "  'for',\n",
       "  'a',\n",
       "  'particular',\n",
       "  'distribution',\n",
       "  'of',\n",
       "  'Hadoop,',\n",
       "  'including'],\n",
       " ['building',\n",
       "  'for',\n",
       "  'particular',\n",
       "  'Hive',\n",
       "  'and',\n",
       "  'Hive',\n",
       "  'Thriftserver',\n",
       "  'distributions.'],\n",
       " [],\n",
       " ['##', 'Configuration'],\n",
       " [],\n",
       " ['Please',\n",
       "  'refer',\n",
       "  'to',\n",
       "  'the',\n",
       "  '[Configuration',\n",
       "  'Guide](http://spark.apache.org/docs/latest/configuration.html)'],\n",
       " ['in',\n",
       "  'the',\n",
       "  'online',\n",
       "  'documentation',\n",
       "  'for',\n",
       "  'an',\n",
       "  'overview',\n",
       "  'on',\n",
       "  'how',\n",
       "  'to',\n",
       "  'configure',\n",
       "  'Spark.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 02-2 : generate a list of words within one level structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line : line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " '',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fast',\n",
       " 'and',\n",
       " 'general',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'system',\n",
       " 'for',\n",
       " 'Big',\n",
       " 'Data.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'in',\n",
       " 'Scala,',\n",
       " 'Java,',\n",
       " 'Python,',\n",
       " 'and',\n",
       " 'R,',\n",
       " 'and',\n",
       " 'an',\n",
       " 'optimized',\n",
       " 'engine',\n",
       " 'that',\n",
       " 'supports',\n",
       " 'general',\n",
       " 'computation',\n",
       " 'graphs',\n",
       " 'for',\n",
       " 'data',\n",
       " 'analysis.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'supports',\n",
       " 'a',\n",
       " 'rich',\n",
       " 'set',\n",
       " 'of',\n",
       " 'higher-level',\n",
       " 'tools',\n",
       " 'including',\n",
       " 'Spark',\n",
       " 'SQL',\n",
       " 'for',\n",
       " 'SQL',\n",
       " 'and',\n",
       " 'DataFrames,',\n",
       " 'MLlib',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'GraphX',\n",
       " 'for',\n",
       " 'graph',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'Spark',\n",
       " 'Streaming',\n",
       " 'for',\n",
       " 'stream',\n",
       " 'processing.',\n",
       " '',\n",
       " '<http://spark.apache.org/>',\n",
       " '',\n",
       " '',\n",
       " '##',\n",
       " 'Online',\n",
       " 'Documentation',\n",
       " '',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'Spark',\n",
       " 'documentation,',\n",
       " 'including',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'guide,',\n",
       " 'on',\n",
       " 'the',\n",
       " '[project',\n",
       " 'web',\n",
       " 'page](http://spark.apache.org/documentation.html)',\n",
       " 'and',\n",
       " '[project',\n",
       " 'wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       " 'This',\n",
       " 'README',\n",
       " 'file',\n",
       " 'only',\n",
       " 'contains',\n",
       " 'basic',\n",
       " 'setup',\n",
       " 'instructions.',\n",
       " '',\n",
       " '##',\n",
       " 'Building',\n",
       " 'Spark',\n",
       " '',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built',\n",
       " 'using',\n",
       " '[Apache',\n",
       " 'Maven](http://maven.apache.org/).',\n",
       " 'To',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'its',\n",
       " 'example',\n",
       " 'programs,',\n",
       " 'run:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'build/mvn',\n",
       " '-DskipTests',\n",
       " 'clean',\n",
       " 'package',\n",
       " '',\n",
       " '(You',\n",
       " 'do',\n",
       " 'not',\n",
       " 'need',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'if',\n",
       " 'you',\n",
       " 'downloaded',\n",
       " 'a',\n",
       " 'pre-built',\n",
       " 'package.)',\n",
       " '',\n",
       " 'You',\n",
       " 'can',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'more',\n",
       " 'than',\n",
       " 'one',\n",
       " 'thread',\n",
       " 'by',\n",
       " 'using',\n",
       " 'the',\n",
       " '-T',\n",
       " 'option',\n",
       " 'with',\n",
       " 'Maven,',\n",
       " 'see',\n",
       " '[\"Parallel',\n",
       " 'builds',\n",
       " 'in',\n",
       " 'Maven',\n",
       " '3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       " 'More',\n",
       " 'detailed',\n",
       " 'documentation',\n",
       " 'is',\n",
       " 'available',\n",
       " 'from',\n",
       " 'the',\n",
       " 'project',\n",
       " 'site,',\n",
       " 'at',\n",
       " '[\"Building',\n",
       " 'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For',\n",
       " 'developing',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'an',\n",
       " 'IDE,',\n",
       " 'see',\n",
       " '[Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)',\n",
       " 'and',\n",
       " '[IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).',\n",
       " '',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Scala',\n",
       " 'Shell',\n",
       " '',\n",
       " 'The',\n",
       " 'easiest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'start',\n",
       " 'using',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'through',\n",
       " 'the',\n",
       " 'Scala',\n",
       " 'shell:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " './bin/spark-shell',\n",
       " '',\n",
       " 'Try',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'return',\n",
       " '1000:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'scala>',\n",
       " 'sc.parallelize(1',\n",
       " 'to',\n",
       " '1000).count()',\n",
       " '',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Python',\n",
       " 'Shell',\n",
       " '',\n",
       " 'Alternatively,',\n",
       " 'if',\n",
       " 'you',\n",
       " 'prefer',\n",
       " 'Python,',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'shell:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " './bin/pyspark',\n",
       " '',\n",
       " 'And',\n",
       " 'run',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'also',\n",
       " 'return',\n",
       " '1000:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '>>>',\n",
       " 'sc.parallelize(range(1000)).count()',\n",
       " '',\n",
       " '##',\n",
       " 'Example',\n",
       " 'Programs',\n",
       " '',\n",
       " 'Spark',\n",
       " 'also',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'several',\n",
       " 'sample',\n",
       " 'programs',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'directory.',\n",
       " 'To',\n",
       " 'run',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them,',\n",
       " 'use',\n",
       " '`./bin/run-example',\n",
       " '<class>',\n",
       " '[params]`.',\n",
       " 'For',\n",
       " 'example:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " '',\n",
       " 'will',\n",
       " 'run',\n",
       " 'the',\n",
       " 'Pi',\n",
       " 'example',\n",
       " 'locally.',\n",
       " '',\n",
       " 'You',\n",
       " 'can',\n",
       " 'set',\n",
       " 'the',\n",
       " 'MASTER',\n",
       " 'environment',\n",
       " 'variable',\n",
       " 'when',\n",
       " 'running',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'a',\n",
       " 'cluster.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'mesos://',\n",
       " 'or',\n",
       " 'spark://',\n",
       " 'URL,',\n",
       " '\"yarn\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'on',\n",
       " 'YARN,',\n",
       " 'and',\n",
       " '\"local\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'one',\n",
       " 'thread,',\n",
       " 'or',\n",
       " '\"local[N]\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'N',\n",
       " 'threads.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'use',\n",
       " 'an',\n",
       " 'abbreviated',\n",
       " 'class',\n",
       " 'name',\n",
       " 'if',\n",
       " 'the',\n",
       " 'class',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'package.',\n",
       " 'For',\n",
       " 'instance:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'MASTER=spark://host:7077',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " '',\n",
       " 'Many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'example',\n",
       " 'programs',\n",
       " 'print',\n",
       " 'usage',\n",
       " 'help',\n",
       " 'if',\n",
       " 'no',\n",
       " 'params',\n",
       " 'are',\n",
       " 'given.',\n",
       " '',\n",
       " '##',\n",
       " 'Running',\n",
       " 'Tests',\n",
       " '',\n",
       " 'Testing',\n",
       " 'first',\n",
       " 'requires',\n",
       " '[building',\n",
       " 'Spark](#building-spark).',\n",
       " 'Once',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built,',\n",
       " 'tests',\n",
       " 'can',\n",
       " 'be',\n",
       " 'run',\n",
       " 'using:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " './dev/run-tests',\n",
       " '',\n",
       " 'Please',\n",
       " 'see',\n",
       " 'the',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " '[run',\n",
       " 'tests',\n",
       " 'for',\n",
       " 'a',\n",
       " 'module,',\n",
       " 'or',\n",
       " 'individual',\n",
       " 'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       " '',\n",
       " '##',\n",
       " 'A',\n",
       " 'Note',\n",
       " 'About',\n",
       " 'Hadoop',\n",
       " 'Versions',\n",
       " '',\n",
       " 'Spark',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'core',\n",
       " 'library',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'HDFS',\n",
       " 'and',\n",
       " 'other',\n",
       " 'Hadoop-supported',\n",
       " 'storage',\n",
       " 'systems.',\n",
       " 'Because',\n",
       " 'the',\n",
       " 'protocols',\n",
       " 'have',\n",
       " 'changed',\n",
       " 'in',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'you',\n",
       " 'must',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'against',\n",
       " 'the',\n",
       " 'same',\n",
       " 'version',\n",
       " 'that',\n",
       " 'your',\n",
       " 'cluster',\n",
       " 'runs.',\n",
       " '',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'build',\n",
       " 'documentation',\n",
       " 'at',\n",
       " '[\"Specifying',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       " 'for',\n",
       " 'detailed',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'building',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'including',\n",
       " 'building',\n",
       " 'for',\n",
       " 'particular',\n",
       " 'Hive',\n",
       " 'and',\n",
       " 'Hive',\n",
       " 'Thriftserver',\n",
       " 'distributions.',\n",
       " '',\n",
       " '##',\n",
       " 'Configuration',\n",
       " '',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " '[Configuration',\n",
       " 'Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in',\n",
       " 'the',\n",
       " 'online',\n",
       " 'documentation',\n",
       " 'for',\n",
       " 'an',\n",
       " 'overview',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'configure',\n",
       " 'Spark.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 02-3 : Return all the words including spark in README.md  (case insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.filter(lambda lines : lines is not u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fast',\n",
       " 'and',\n",
       " 'general',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'system',\n",
       " 'for',\n",
       " 'Big',\n",
       " 'Data.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'in',\n",
       " 'Scala,',\n",
       " 'Java,',\n",
       " 'Python,',\n",
       " 'and',\n",
       " 'R,',\n",
       " 'and',\n",
       " 'an',\n",
       " 'optimized',\n",
       " 'engine',\n",
       " 'that',\n",
       " 'supports',\n",
       " 'general',\n",
       " 'computation',\n",
       " 'graphs',\n",
       " 'for',\n",
       " 'data',\n",
       " 'analysis.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'supports',\n",
       " 'a',\n",
       " 'rich',\n",
       " 'set',\n",
       " 'of',\n",
       " 'higher-level',\n",
       " 'tools',\n",
       " 'including',\n",
       " 'Spark',\n",
       " 'SQL',\n",
       " 'for',\n",
       " 'SQL',\n",
       " 'and',\n",
       " 'DataFrames,',\n",
       " 'MLlib',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'GraphX',\n",
       " 'for',\n",
       " 'graph',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'Spark',\n",
       " 'Streaming',\n",
       " 'for',\n",
       " 'stream',\n",
       " 'processing.',\n",
       " '<http://spark.apache.org/>',\n",
       " '##',\n",
       " 'Online',\n",
       " 'Documentation',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'Spark',\n",
       " 'documentation,',\n",
       " 'including',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'guide,',\n",
       " 'on',\n",
       " 'the',\n",
       " '[project',\n",
       " 'web',\n",
       " 'page](http://spark.apache.org/documentation.html)',\n",
       " 'and',\n",
       " '[project',\n",
       " 'wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       " 'This',\n",
       " 'README',\n",
       " 'file',\n",
       " 'only',\n",
       " 'contains',\n",
       " 'basic',\n",
       " 'setup',\n",
       " 'instructions.',\n",
       " '##',\n",
       " 'Building',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built',\n",
       " 'using',\n",
       " '[Apache',\n",
       " 'Maven](http://maven.apache.org/).',\n",
       " 'To',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'its',\n",
       " 'example',\n",
       " 'programs,',\n",
       " 'run:',\n",
       " 'build/mvn',\n",
       " '-DskipTests',\n",
       " 'clean',\n",
       " 'package',\n",
       " '(You',\n",
       " 'do',\n",
       " 'not',\n",
       " 'need',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'if',\n",
       " 'you',\n",
       " 'downloaded',\n",
       " 'a',\n",
       " 'pre-built',\n",
       " 'package.)',\n",
       " 'You',\n",
       " 'can',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'more',\n",
       " 'than',\n",
       " 'one',\n",
       " 'thread',\n",
       " 'by',\n",
       " 'using',\n",
       " 'the',\n",
       " '-T',\n",
       " 'option',\n",
       " 'with',\n",
       " 'Maven,',\n",
       " 'see',\n",
       " '[\"Parallel',\n",
       " 'builds',\n",
       " 'in',\n",
       " 'Maven',\n",
       " '3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       " 'More',\n",
       " 'detailed',\n",
       " 'documentation',\n",
       " 'is',\n",
       " 'available',\n",
       " 'from',\n",
       " 'the',\n",
       " 'project',\n",
       " 'site,',\n",
       " 'at',\n",
       " '[\"Building',\n",
       " 'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For',\n",
       " 'developing',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'an',\n",
       " 'IDE,',\n",
       " 'see',\n",
       " '[Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)',\n",
       " 'and',\n",
       " '[IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Scala',\n",
       " 'Shell',\n",
       " 'The',\n",
       " 'easiest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'start',\n",
       " 'using',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'through',\n",
       " 'the',\n",
       " 'Scala',\n",
       " 'shell:',\n",
       " './bin/spark-shell',\n",
       " 'Try',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'return',\n",
       " '1000:',\n",
       " 'scala>',\n",
       " 'sc.parallelize(1',\n",
       " 'to',\n",
       " '1000).count()',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Python',\n",
       " 'Shell',\n",
       " 'Alternatively,',\n",
       " 'if',\n",
       " 'you',\n",
       " 'prefer',\n",
       " 'Python,',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'shell:',\n",
       " './bin/pyspark',\n",
       " 'And',\n",
       " 'run',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'also',\n",
       " 'return',\n",
       " '1000:',\n",
       " '>>>',\n",
       " 'sc.parallelize(range(1000)).count()',\n",
       " '##',\n",
       " 'Example',\n",
       " 'Programs',\n",
       " 'Spark',\n",
       " 'also',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'several',\n",
       " 'sample',\n",
       " 'programs',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'directory.',\n",
       " 'To',\n",
       " 'run',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them,',\n",
       " 'use',\n",
       " '`./bin/run-example',\n",
       " '<class>',\n",
       " '[params]`.',\n",
       " 'For',\n",
       " 'example:',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " 'will',\n",
       " 'run',\n",
       " 'the',\n",
       " 'Pi',\n",
       " 'example',\n",
       " 'locally.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'set',\n",
       " 'the',\n",
       " 'MASTER',\n",
       " 'environment',\n",
       " 'variable',\n",
       " 'when',\n",
       " 'running',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'a',\n",
       " 'cluster.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'mesos://',\n",
       " 'or',\n",
       " 'spark://',\n",
       " 'URL,',\n",
       " '\"yarn\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'on',\n",
       " 'YARN,',\n",
       " 'and',\n",
       " '\"local\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'one',\n",
       " 'thread,',\n",
       " 'or',\n",
       " '\"local[N]\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'N',\n",
       " 'threads.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'use',\n",
       " 'an',\n",
       " 'abbreviated',\n",
       " 'class',\n",
       " 'name',\n",
       " 'if',\n",
       " 'the',\n",
       " 'class',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'package.',\n",
       " 'For',\n",
       " 'instance:',\n",
       " 'MASTER=spark://host:7077',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " 'Many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'example',\n",
       " 'programs',\n",
       " 'print',\n",
       " 'usage',\n",
       " 'help',\n",
       " 'if',\n",
       " 'no',\n",
       " 'params',\n",
       " 'are',\n",
       " 'given.',\n",
       " '##',\n",
       " 'Running',\n",
       " 'Tests',\n",
       " 'Testing',\n",
       " 'first',\n",
       " 'requires',\n",
       " '[building',\n",
       " 'Spark](#building-spark).',\n",
       " 'Once',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built,',\n",
       " 'tests',\n",
       " 'can',\n",
       " 'be',\n",
       " 'run',\n",
       " 'using:',\n",
       " './dev/run-tests',\n",
       " 'Please',\n",
       " 'see',\n",
       " 'the',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " '[run',\n",
       " 'tests',\n",
       " 'for',\n",
       " 'a',\n",
       " 'module,',\n",
       " 'or',\n",
       " 'individual',\n",
       " 'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       " '##',\n",
       " 'A',\n",
       " 'Note',\n",
       " 'About',\n",
       " 'Hadoop',\n",
       " 'Versions',\n",
       " 'Spark',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'core',\n",
       " 'library',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'HDFS',\n",
       " 'and',\n",
       " 'other',\n",
       " 'Hadoop-supported',\n",
       " 'storage',\n",
       " 'systems.',\n",
       " 'Because',\n",
       " 'the',\n",
       " 'protocols',\n",
       " 'have',\n",
       " 'changed',\n",
       " 'in',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'you',\n",
       " 'must',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'against',\n",
       " 'the',\n",
       " 'same',\n",
       " 'version',\n",
       " 'that',\n",
       " 'your',\n",
       " 'cluster',\n",
       " 'runs.',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'build',\n",
       " 'documentation',\n",
       " 'at',\n",
       " '[\"Specifying',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       " 'for',\n",
       " 'detailed',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'building',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'including',\n",
       " 'building',\n",
       " 'for',\n",
       " 'particular',\n",
       " 'Hive',\n",
       " 'and',\n",
       " 'Hive',\n",
       " 'Thriftserver',\n",
       " 'distributions.',\n",
       " '##',\n",
       " 'Configuration',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " '[Configuration',\n",
       " 'Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in',\n",
       " 'the',\n",
       " 'online',\n",
       " 'documentation',\n",
       " 'for',\n",
       " 'an',\n",
       " 'overview',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'configure',\n",
       " 'Spark.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_words = words.filter(lambda lines : \"spark\" in lines.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " '<http://spark.apache.org/>',\n",
       " 'Spark',\n",
       " 'page](http://spark.apache.org/documentation.html)',\n",
       " 'wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'Spark',\n",
       " '[Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)',\n",
       " '[IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).',\n",
       " 'Spark',\n",
       " './bin/spark-shell',\n",
       " './bin/pyspark',\n",
       " 'Spark',\n",
       " 'SparkPi',\n",
       " 'spark://',\n",
       " 'MASTER=spark://host:7077',\n",
       " 'SparkPi',\n",
       " 'Spark](#building-spark).',\n",
       " 'Spark',\n",
       " 'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       " 'Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'Spark.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
